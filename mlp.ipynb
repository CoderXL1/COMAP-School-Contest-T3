{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-19T06:12:43.847603Z",
     "start_time": "2026-01-19T06:12:43.828659Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T14:14:21.253668Z",
     "start_time": "2026-01-19T14:14:20.880202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = '/Users/aplle/Code/MathModeling/SC/dataset1'\n",
    "MODEL_DIR = '/Users/aplle/Code/MathModeling/SC/models'\n",
    "OUT_DIR = '/Users/aplle/Code/MathModeling/SC/results'"
   ],
   "id": "8e64e13e3b05f956",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T07:50:22.655410Z",
     "start_time": "2026-01-19T07:50:20.127763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "eval_df = pd.read_csv(f\"{DATA_DIR}/eval.csv\")\n",
    "test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
    "\n",
    "POWER_DIVISOR = 1000.0\n",
    "YEAR_MIN = 2016\n",
    "YEAR_MAX = 2018\n",
    "\n",
    "def is_leap(yr:int):\n",
    "    if yr % 400 == 0:\n",
    "        return True\n",
    "    elif yr % 100 == 0:\n",
    "        return False\n",
    "    elif yr % 4 == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_ndays(yr:int):\n",
    "    return 366 if is_leap(yr) else 365\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df.dropna(subset=['Power (kW)'], inplace=True)\n",
    "    df['Power (kW)'] = df['Power (kW)'] / POWER_DIVISOR\n",
    "    df.rename(columns={'Power (kW)': 'Power'}, inplace=True)\n",
    "\n",
    "    df['Days_from_NYD'] = df['Days_from_NYD'] / df['Year'].apply(get_ndays)\n",
    "\n",
    "    df['Year'] = df['Year'] - YEAR_MIN\n",
    "    df['Year'] = df['Year'] / (YEAR_MAX - YEAR_MIN)\n",
    "\n",
    "    # BLIND THE YEAR INFORMATION\n",
    "    # df['Year'] = 0.0\n",
    "\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S')\n",
    "    df['Time'] = df['Time'].dt.hour * 60 + df['Time'].dt.minute\n",
    "    df['Time'] = df['Time'] / (24 * 60)\n",
    "\n",
    "    df.drop(['Day', 'Span'], axis=1, inplace=True)\n",
    "\n",
    "    df['Month'] = df['Month'] / 12.0\n",
    "\n",
    "    return pd.get_dummies(df, columns=['Weekday', 'Region'], drop_first=True)\n",
    "\n",
    "train_df = preprocess_df(train_df)\n",
    "# keep canonical columns from train\n",
    "_train_columns = train_df.columns.copy()\n",
    "\n",
    "eval_df = preprocess_df(eval_df)\n",
    "# reindex eval to have same columns as train (fill missing dummies with 0)\n",
    "eval_df = eval_df.reindex(columns=_train_columns, fill_value=0)\n",
    "\n",
    "test_df = preprocess_df(test_df)\n",
    "# reindex test to have same columns as train\n",
    "test_df = test_df.reindex(columns=_train_columns, fill_value=0)\n"
   ],
   "id": "52afddba2aae7bb2",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T05:18:14.009359Z",
     "start_time": "2026-01-19T05:18:13.898361Z"
    }
   },
   "cell_type": "code",
   "source": "test_df.info()",
   "id": "de6df831c5e71728",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 139981 entries, 0 to 141031\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   Time                139981 non-null  float64\n",
      " 1   Power               139981 non-null  float64\n",
      " 2   Year                139981 non-null  float64\n",
      " 3   Month               139981 non-null  float64\n",
      " 4   Days_from_NYD       139981 non-null  float64\n",
      " 5   Weekday_1           139981 non-null  bool   \n",
      " 6   Weekday_2           139981 non-null  bool   \n",
      " 7   Weekday_3           139981 non-null  bool   \n",
      " 8   Weekday_4           139981 non-null  bool   \n",
      " 9   Weekday_5           139981 non-null  bool   \n",
      " 10  Weekday_6           139981 non-null  bool   \n",
      " 11  Region_Office       139981 non-null  bool   \n",
      " 12  Region_Public       139981 non-null  bool   \n",
      " 13  Region_Residential  139981 non-null  bool   \n",
      "dtypes: bool(9), float64(5)\n",
      "memory usage: 7.6 MB\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T05:18:18.849217Z",
     "start_time": "2026-01-19T05:18:17.776498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_cols = train_df.drop(columns=['Power']).columns\n",
    "input_dim = len(feature_cols)\n",
    "\n",
    "X_train = torch.tensor(train_df[feature_cols].to_numpy().astype(np.float64), dtype=torch.float32)\n",
    "y_train = torch.tensor(train_df['Power'].to_numpy().astype(np.float64).reshape(-1, 1), dtype=torch.float32)\n",
    "train_dataset = data.TensorDataset(X_train, y_train)\n",
    "\n",
    "X_eval = torch.tensor(eval_df[feature_cols].to_numpy().astype(np.float64), dtype=torch.float32)\n",
    "y_eval = torch.tensor(eval_df['Power'].to_numpy().astype(np.float64).reshape(-1, 1), dtype=torch.float32)\n",
    "eval_dataset = data.TensorDataset(X_eval, y_eval)\n",
    "\n",
    "X_test = torch.tensor(test_df[feature_cols].to_numpy().astype(np.float64), dtype=torch.float32)\n",
    "y_test = torch.tensor(test_df['Power'].to_numpy().astype(np.float64).reshape(-1, 1), dtype=torch.float32)\n",
    "test_dataset = data.TensorDataset(X_test, y_test)"
   ],
   "id": "7d51fb8d31f89bf4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T07:50:33.526615Z",
     "start_time": "2026-01-19T07:50:33.482885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mlp2 = mlp\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(input_dim, 64),\n",
    "    nn.ReLU(),\n",
    "    # nn.Dropout(0.1),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(32, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(8, 1)\n",
    ")"
   ],
   "id": "3f59b788201cc885",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T07:50:36.630880Z",
     "start_time": "2026-01-19T07:50:36.573862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_iter(model, dataset, opt, loss, batch_size=32):\n",
    "    model.train()\n",
    "    loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in loader:\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        l = loss(y_pred, y_batch)\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "        total_loss += l.item() * X_batch.size(0)\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "def evaluate(model, dataset, loss, batch_size=32):\n",
    "    model.eval()\n",
    "    loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            l = loss(y_pred, y_batch)\n",
    "            total_loss += l.item() * X_batch.size(0)\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "def train(model, train_dataset, eval_dataset, epochs=100, batch_size=32, lr=1e-3, weight_decay=0.0, patience=10, checkpoint_path=None):\n",
    "    \"\"\"Train with simple early stopping and optional checkpointing.\n",
    "\n",
    "    Args:\n",
    "        model: torch.nn.Module\n",
    "        train_dataset, eval_dataset: Dataset\n",
    "        epochs, batch_size, lr, weight_decay: optimizer params\n",
    "        patience: stop if eval loss doesn't improve for this many epochs\n",
    "        checkpoint_path: if provided, save best model to this path\n",
    "\n",
    "    Returns:\n",
    "        best_eval_loss\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_eval = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_iter(model, train_dataset, opt, loss_fn, batch_size)\n",
    "        eval_loss = evaluate(model, eval_dataset, loss_fn, batch_size)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Eval Loss: {eval_loss:.6f}\")\n",
    "\n",
    "        if eval_loss < best_eval - 1e-9:\n",
    "            best_eval = eval_loss\n",
    "            epochs_no_improve = 0\n",
    "            # store best weights in memory\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            if checkpoint_path:\n",
    "                try:\n",
    "                    torch.save(best_state, checkpoint_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs (no improvement for {patience} epochs). Best eval: {best_eval:.6f}\")\n",
    "            break\n",
    "\n",
    "    # restore best weights if we have them\n",
    "    if best_state is not None:\n",
    "        try:\n",
    "            model.load_state_dict(best_state)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return best_eval\n"
   ],
   "id": "bd22ca7a8a2038be",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T08:03:57.367652Z",
     "start_time": "2026-01-19T07:50:41.828504Z"
    }
   },
   "cell_type": "code",
   "source": "train(mlp, train_dataset, eval_dataset, epochs=50, batch_size=32, lr=1e-4, weight_decay=1e-6, patience=5, checkpoint_path=os.path.join(MODEL_DIR,'mlp_checkpoint.pth'))",
   "id": "d23fdb4e01d7fb29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.036924, Eval Loss: 0.019688\n",
      "Epoch 2/50, Train Loss: 0.023492, Eval Loss: 0.019517\n",
      "Epoch 3/50, Train Loss: 0.022042, Eval Loss: 0.018159\n",
      "Epoch 4/50, Train Loss: 0.020945, Eval Loss: 0.017349\n",
      "Epoch 5/50, Train Loss: 0.019772, Eval Loss: 0.017858\n",
      "Epoch 6/50, Train Loss: 0.018204, Eval Loss: 0.016591\n",
      "Epoch 7/50, Train Loss: 0.017079, Eval Loss: 0.015860\n",
      "Epoch 8/50, Train Loss: 0.016564, Eval Loss: 0.015550\n",
      "Epoch 9/50, Train Loss: 0.016269, Eval Loss: 0.016044\n",
      "Epoch 10/50, Train Loss: 0.016024, Eval Loss: 0.015018\n",
      "Epoch 11/50, Train Loss: 0.015874, Eval Loss: 0.015835\n",
      "Epoch 12/50, Train Loss: 0.015699, Eval Loss: 0.014584\n",
      "Epoch 13/50, Train Loss: 0.015560, Eval Loss: 0.014988\n",
      "Epoch 14/50, Train Loss: 0.015484, Eval Loss: 0.014503\n",
      "Epoch 15/50, Train Loss: 0.015339, Eval Loss: 0.015237\n",
      "Epoch 16/50, Train Loss: 0.015270, Eval Loss: 0.014091\n",
      "Epoch 17/50, Train Loss: 0.015209, Eval Loss: 0.014685\n",
      "Epoch 18/50, Train Loss: 0.015122, Eval Loss: 0.014887\n",
      "Epoch 19/50, Train Loss: 0.015107, Eval Loss: 0.016041\n",
      "Epoch 20/50, Train Loss: 0.015047, Eval Loss: 0.015166\n",
      "Epoch 21/50, Train Loss: 0.015017, Eval Loss: 0.015250\n",
      "Early stopping after 21 epochs (no improvement for 5 epochs). Best eval: 0.014091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.014091205044283598"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T14:14:41.588907Z",
     "start_time": "2026-01-19T14:14:41.376338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def unembedding(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return model(X) * POWER_DIVISOR\n",
    "\n",
    "\n",
    "def test(model, test_dataset=test_dataset, max_count=None):\n",
    "    \"\"\"Return a DataFrame with true and predicted power (kW) for up to max_count examples.\n",
    "\n",
    "    This function uses the tensors already created earlier (X_test/y_test stored in the notebook) via the provided dataset.\n",
    "    \"\"\"\n",
    "    # Build a loader so we can slice the first `max_count` examples safely.\n",
    "    loader = data.DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "    seen = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            if max_count is None:\n",
    "                X_parts.append(X_batch)\n",
    "                y_parts.append(y_batch)\n",
    "            else:\n",
    "                need = max_count - seen\n",
    "                if need <= 0:\n",
    "                    break\n",
    "                if X_batch.size(0) > need:\n",
    "                    X_parts.append(X_batch[:need])\n",
    "                    y_parts.append(y_batch[:need])\n",
    "                    seen += need\n",
    "                    break\n",
    "                else:\n",
    "                    X_parts.append(X_batch)\n",
    "                    y_parts.append(y_batch)\n",
    "                    seen += X_batch.size(0)\n",
    "\n",
    "    if len(X_parts) == 0:\n",
    "        return pd.DataFrame(columns=['Power (kW)', 'Predicted Power (kW)'])\n",
    "\n",
    "    X_all = torch.cat(X_parts, dim=0)\n",
    "    y_all = torch.cat(y_parts, dim=0)\n",
    "\n",
    "    y_pred = unembedding(model, X_all).squeeze().cpu().numpy()\n",
    "    y_true = (y_all.squeeze().cpu().numpy()) * POWER_DIVISOR\n",
    "\n",
    "    ret = pd.DataFrame({'Power (kW)': y_true, 'Predicted Power (kW)': y_pred})\n",
    "    return ret"
   ],
   "id": "592e436008fc3945",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T14:18:47.368962Z",
     "start_time": "2026-01-19T14:18:44.516568Z"
    }
   },
   "cell_type": "code",
   "source": "result = test(mlp, test_dataset)",
   "id": "dc2168d36151af3e",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Region'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3652\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Region'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m result \u001B[38;5;241m=\u001B[39m test(mlp, test_dataset)\n\u001B[0;32m----> 2\u001B[0m result[\u001B[43mtest_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRegion\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCommercial\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# result.to_csv(os.path.join(OUT_DIR, 'mlp_eval_results.csv'), index=False)\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3760\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3761\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3763\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3655\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3656\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3657\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3658\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3659\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3660\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Region'"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T14:38:46.269479Z",
     "start_time": "2026-01-19T14:38:45.667838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# result = pd.DataFrame(result)\n",
    "test_df['Region_Commercial'] = ~(test_df['Region_Office'] | test_df['Region_Public'] | test_df['Region_Residential'])\n",
    "result[(test_df['Region_Office'] == True).reset_index(drop=True)].to_csv(os.path.join(OUT_DIR, 'mlp_test_results_office.csv'), index=False)\n",
    "result[(test_df['Region_Public'] == True).reset_index(drop=True)].to_csv(os.path.join(OUT_DIR, 'mlp_test_results_public.csv'), index=False)\n",
    "result[(test_df['Region_Residential'] == True).reset_index(drop=True)].to_csv(os.path.join(OUT_DIR, 'mlp_test_results_residential.csv'), index=False)\n",
    "result[(test_df['Region_Commercial'] == True).reset_index(drop=True)].to_csv(os.path.join(OUT_DIR, 'mlp_test_results_commercial.csv'), index=False)"
   ],
   "id": "b241a97174662c80",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T04:53:26.818989Z",
     "start_time": "2026-01-19T04:53:26.682951Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71,
   "source": "input_dim",
   "id": "2482eddfcacb5a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12d6634be299a152"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
